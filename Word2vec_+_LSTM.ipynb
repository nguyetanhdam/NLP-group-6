{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOCYS-phxkUD",
        "outputId": "5f05549e-70fd-47d2-9a04-c02d71244619"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting underthesea\n",
            "  Downloading underthesea-6.8.0-py3-none-any.whl (20.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from underthesea) (8.1.7)\n",
            "Collecting python-crfsuite>=0.9.6 (from underthesea)\n",
            "  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from underthesea) (3.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from underthesea) (4.66.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from underthesea) (2.31.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.3.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.2.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from underthesea) (6.0.1)\n",
            "Collecting underthesea-core==1.0.4 (from underthesea)\n",
            "  Downloading underthesea_core-1.0.4-cp310-cp310-manylinux2010_x86_64.whl (657 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m657.8/657.8 kB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->underthesea) (2023.6.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2023.7.22)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.11.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (3.2.0)\n",
            "Installing collected packages: underthesea-core, python-crfsuite, underthesea\n",
            "Successfully installed python-crfsuite-0.9.9 underthesea-6.8.0 underthesea-core-1.0.4\n"
          ]
        }
      ],
      "source": [
        "!pip install underthesea"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aB-B05Ehxp9s"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import regex\n",
        "from sklearn.model_selection import train_test_split\n",
        "from underthesea import word_tokenize\n",
        "import gensim\n",
        "import pandas as pd\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ldxf2DgExuZl"
      },
      "outputs": [],
      "source": [
        "with open(f\"/content/stop_word_vn.txt\", encoding=\"utf8\") as f:\n",
        "    stop_word_pre = f.read().splitlines()\n",
        "\n",
        "\n",
        "def no_accent_vietnamese(s):\n",
        "    s = regex.sub(r'[àáạảãâầấậẩẫăằắặẳẵ]', 'a', s)\n",
        "    s = regex.sub(r'[ÀÁẠẢÃĂẰẮẶẲẴÂẦẤẬẨẪ]', 'A', s)\n",
        "    s = regex.sub(r'[èéẹẻẽêềếệểễ]', 'e', s)\n",
        "    s = regex.sub(r'[ÈÉẸẺẼÊỀẾỆỂỄ]', 'E', s)\n",
        "    s = regex.sub(r'[òóọỏõôồốộổỗơờớợởỡ]', 'o', s)\n",
        "    s = regex.sub(r'[ÒÓỌỎÕÔỒỐỘỔỖƠỜỚỢỞỠ]', 'O', s)\n",
        "    s = regex.sub(r'[ìíịỉĩ]', 'i', s)\n",
        "    s = regex.sub(r'[ÌÍỊỈĨ]', 'I', s)\n",
        "    s = regex.sub(r'[ùúụủũưừứựửữ]', 'u', s)\n",
        "    s = regex.sub(r'[ƯỪỨỰỬỮÙÚỤỦŨ]', 'U', s)\n",
        "    s = regex.sub(r'[ỳýỵỷỹ]', 'y', s)\n",
        "    s = regex.sub(r'[ỲÝỴỶỸ]', 'Y', s)\n",
        "    s = regex.sub(r'[Đ]', 'D', s)\n",
        "    s = regex.sub(r'[đ]', 'd', s)\n",
        "    return s\n",
        "\n",
        "stop_word = set()\n",
        "for a in stop_word_pre:\n",
        "  stop_word.add(a)\n",
        "  stop_word.add(no_accent_vietnamese(a))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tE4rW4yJESyW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_59Kfgp-xu4k"
      },
      "outputs": [],
      "source": [
        "VN_DATA_DIRECT = '/content/sentiment_analysis_train.v1.0.txt'\n",
        "\n",
        "# with open(f\"/content/stop_word_vn.txt\", encoding=\"utf8\") as f:\n",
        "#     stop_word = f.read().splitlines()\n",
        "\n",
        "\n",
        "def remove_character_not_ness(document):\n",
        "    document = regex.sub(r'[^\\s\\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ_]', ' ', document)\n",
        "    document = regex.sub(r'[^\\D]', ' ', document)\n",
        "    document = regex.sub(r'\\s+', ' ', document).strip()\n",
        "    return document\n",
        "\n",
        "\n",
        "def remove_stopwords(document):\n",
        "    words = []\n",
        "    for word in document.strip().split():\n",
        "        if word not in stop_word:\n",
        "            words.append(word)\n",
        "    return ' '.join(words)\n",
        "\n",
        "\n",
        "def text_preprocess(document):\n",
        "    document = document.lower()\n",
        "    document = word_tokenize(document, format=\"text\")\n",
        "    document = remove_character_not_ness(document)\n",
        "    # document = remove_stopwords(document)\n",
        "    return document\n",
        "\n",
        "\n",
        "text = []\n",
        "label = []\n",
        "\n",
        "\n",
        "def load_training_data(\n",
        "        vn_directory\n",
        "):\n",
        "    with open(f\"{vn_directory}\", encoding=\"utf8\") as f:\n",
        "        data_set = f.read().splitlines()\n",
        "        for data in data_set:\n",
        "            document = data\n",
        "            tmp = document.split(\" \", 1)\n",
        "            tmp[1] = text_preprocess(tmp[1])\n",
        "            label.append(tmp[0])\n",
        "            text.append(tmp[1])\n",
        "            s = no_accent_vietnamese(tmp[1]).replace('_', ' ')\n",
        "            if tmp[1] != s:\n",
        "                label.append(tmp[0])\n",
        "                text.append(s)\n",
        "\n",
        "\n",
        "load_training_data(VN_DATA_DIRECT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-i2cmdBYxw7V",
        "outputId": "ab8c8cbc-1419-4593-c728-5a04477185c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1283\n"
          ]
        }
      ],
      "source": [
        "count = 0\n",
        "for a in text:\n",
        "  tmp = a.split(\" \");\n",
        "  count = max(count, len(tmp))\n",
        "print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vrgqHmGTxyi0"
      },
      "outputs": [],
      "source": [
        "def txtTokenizer(texts):\n",
        "    tokenizer = Tokenizer(num_words = count,lower=True)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    return tokenizer, word_index\n",
        "\n",
        "tokenizer, word_index = txtTokenizer(text)\n",
        "X = tokenizer.texts_to_sequences(text)\n",
        "X = pad_sequences(X, count)\n",
        "y = pd.get_dummies(label)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=101)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Qgwc-is3xzB6"
      },
      "outputs": [],
      "source": [
        "sentences = [[item.lower() for item in doc.split()] for doc in text]\n",
        "word_model = gensim.models.Word2Vec(sentences, vector_size=300, min_count=1, epochs=10)\n",
        "embedding_matrix = np.zeros((len(word_model.wv.key_to_index) + 1, 300))\n",
        "for i, vec in enumerate(word_model.wv.vectors):\n",
        "    embedding_matrix[i] = vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jd6gqBNIZR0",
        "outputId": "bd440c72-9553-494a-bbd8-8db4917bdba0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('hẹp', 0.8410004377365112), ('chật', 0.7688353657722473), ('cũ_kỹ', 0.7687984108924866), ('chật_hẹp', 0.7677358984947205), ('bí', 0.7647214531898499), ('chật_chội', 0.7599613070487976), ('bẩn', 0.7582138180732727), ('ẩm_thấp', 0.7498104572296143), ('dơ', 0.7403759956359863), ('nhỏ_hẹp', 0.7168053984642029)]\n"
          ]
        }
      ],
      "source": [
        "print(word_model.wv.most_similar('xấu'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dmQfLpEOx_7_"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Assuming you already have 'sentences' and 'word_model' defined\n",
        "\n",
        "embedding_matrix = np.zeros((len(word_model.wv.key_to_index) + 1, 300))\n",
        "for i, vec in enumerate(word_model.wv.vectors):\n",
        "    embedding_matrix[i] = vec\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(word_model.wv.key_to_index) + 1, 300, input_length=X.shape[1], weights=[embedding_matrix], trainable=False))\n",
        "model.add(LSTM(300, return_sequences=False))\n",
        "model.add(Dense(y.shape[1], activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LiklcTokA0ha"
      },
      "outputs": [],
      "source": [
        "test = []\n",
        "with open(f\"/content/sentiment_analysis_test_unlabel.v1.0.txt\", encoding=\"utf8\") as f:\n",
        "  data_set = f.read().splitlines()\n",
        "  for data in data_set:\n",
        "    data = text_preprocess(data)\n",
        "    test.append(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWqFsfGeyADf",
        "outputId": "450fb599-43cb-4166-a80d-d5e15697d29b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "633/633 [==============================] - 24s 30ms/step\n",
            "__label__rat_kem\n"
          ]
        }
      ],
      "source": [
        "seq = tokenizer.texts_to_sequences(test)\n",
        "padded = pad_sequences(seq, count)\n",
        "pred = model.predict(padded)\n",
        "labels = ['__label__kem', '__label__rat_kem', '__label__tot', '__label__trung_binh', '__label__xuat_sac']\n",
        "y = pd.get_dummies(label)\n",
        "print(labels[np.argmax(pred[1])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "f2JWiWtyDRUh"
      },
      "outputs": [],
      "source": [
        "for p in pred:\n",
        "    ff = open(f\"/content/result-multi.txt\", \"a\", encoding=\"utf8\")\n",
        "    ff.write(labels[np.argmax(p)] + '\\n')\n",
        "    ff.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}