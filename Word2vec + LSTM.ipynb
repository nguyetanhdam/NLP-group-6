{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOCYS-phxkUD",
        "outputId": "de6ccf92-e4eb-4792-cd76-32116d5bf070"
      },
      "outputs": [],
      "source": [
        "!pip install underthesea"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aB-B05Ehxp9s"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import regex\n",
        "from sklearn.model_selection import train_test_split\n",
        "from underthesea import word_tokenize\n",
        "import gensim\n",
        "import pandas as pd\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ldxf2DgExuZl"
      },
      "outputs": [],
      "source": [
        "with open(f\"/content/stop_word_vn.txt\", encoding=\"utf8\") as f:\n",
        "    stop_word_pre = f.read().splitlines()\n",
        "\n",
        "\n",
        "def no_accent_vietnamese(s):\n",
        "    s = regex.sub(r'[àáạảãâầấậẩẫăằắặẳẵ]', 'a', s)\n",
        "    s = regex.sub(r'[ÀÁẠẢÃĂẰẮẶẲẴÂẦẤẬẨẪ]', 'A', s)\n",
        "    s = regex.sub(r'[èéẹẻẽêềếệểễ]', 'e', s)\n",
        "    s = regex.sub(r'[ÈÉẸẺẼÊỀẾỆỂỄ]', 'E', s)\n",
        "    s = regex.sub(r'[òóọỏõôồốộổỗơờớợởỡ]', 'o', s)\n",
        "    s = regex.sub(r'[ÒÓỌỎÕÔỒỐỘỔỖƠỜỚỢỞỠ]', 'O', s)\n",
        "    s = regex.sub(r'[ìíịỉĩ]', 'i', s)\n",
        "    s = regex.sub(r'[ÌÍỊỈĨ]', 'I', s)\n",
        "    s = regex.sub(r'[ùúụủũưừứựửữ]', 'u', s)\n",
        "    s = regex.sub(r'[ƯỪỨỰỬỮÙÚỤỦŨ]', 'U', s)\n",
        "    s = regex.sub(r'[ỳýỵỷỹ]', 'y', s)\n",
        "    s = regex.sub(r'[ỲÝỴỶỸ]', 'Y', s)\n",
        "    s = regex.sub(r'[Đ]', 'D', s)\n",
        "    s = regex.sub(r'[đ]', 'd', s)\n",
        "    return s\n",
        "\n",
        "stop_word = set()\n",
        "for a in stop_word_pre:\n",
        "  stop_word.add(a)\n",
        "  stop_word.add(no_accent_vietnamese(a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_59Kfgp-xu4k"
      },
      "outputs": [],
      "source": [
        "VN_DATA_DIRECT = '/content/sentiment_analysis_train.v1.0.txt'\n",
        "\n",
        "# with open(f\"/content/stop_word_vn.txt\", encoding=\"utf8\") as f:\n",
        "#     stop_word = f.read().splitlines()\n",
        "\n",
        "\n",
        "def remove_character_not_ness(document):\n",
        "    document = regex.sub(r'[^\\s\\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ_]', ' ', document)\n",
        "    document = regex.sub(r'[^\\D]', ' ', document)\n",
        "    document = regex.sub(r'\\s+', ' ', document).strip()\n",
        "    return document\n",
        "\n",
        "\n",
        "def remove_stopwords(document):\n",
        "    words = []\n",
        "    for word in document.strip().split():\n",
        "        if word not in stop_word:\n",
        "            words.append(word)\n",
        "    return ' '.join(words)\n",
        "\n",
        "\n",
        "def text_preprocess(document):\n",
        "    document = document.lower()\n",
        "    document = word_tokenize(document, format=\"text\")\n",
        "    document = remove_character_not_ness(document)\n",
        "    # document = remove_stopwords(document)\n",
        "    return document\n",
        "\n",
        "\n",
        "text = []\n",
        "label = []\n",
        "\n",
        "\n",
        "def load_training_data(\n",
        "        vn_directory\n",
        "):\n",
        "    with open(f\"{vn_directory}\", encoding=\"utf8\") as f:\n",
        "        data_set = f.read().splitlines()\n",
        "        for data in data_set:\n",
        "            document = data\n",
        "            tmp = document.split(\" \", 1)\n",
        "            tmp[1] = text_preprocess(tmp[1])\n",
        "            label.append(tmp[0])\n",
        "            text.append(tmp[1])\n",
        "            s = no_accent_vietnamese(tmp[1]).replace('_', ' ')\n",
        "            if tmp[1] != s:\n",
        "                label.append(tmp[0])\n",
        "                text.append(s)\n",
        "\n",
        "\n",
        "load_training_data(VN_DATA_DIRECT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-i2cmdBYxw7V",
        "outputId": "afec8629-ed72-489c-d3c2-14f9278ea274"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1283\n"
          ]
        }
      ],
      "source": [
        "count = 0\n",
        "for a in text:\n",
        "  tmp = a.split(\" \");\n",
        "  count = max(count, len(tmp))\n",
        "print(count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vrgqHmGTxyi0"
      },
      "outputs": [],
      "source": [
        "def txtTokenizer(texts):\n",
        "    tokenizer = Tokenizer(num_words = count,lower=True)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    return tokenizer, word_index\n",
        "\n",
        "tokenizer, word_index = txtTokenizer(text)\n",
        "X = tokenizer.texts_to_sequences(text)\n",
        "X = pad_sequences(X, count)\n",
        "y = pd.get_dummies(label)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=101)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Qgwc-is3xzB6"
      },
      "outputs": [],
      "source": [
        "sentences = [[item.lower() for item in doc.split()] for doc in text]\n",
        "word_model = gensim.models.Word2Vec(sentences, size=300, min_count=1, iter=10)\n",
        "embedding_matrix = np.zeros((len(word_model.wv.vocab) + 1, 300))\n",
        "for i, vec in enumerate(word_model.wv.vectors):\n",
        "  embedding_matrix[i] = vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jd6gqBNIZR0",
        "outputId": "dc4592ab-7717-4308-9bf7-1179232d9655"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('hẹp', 0.8238487243652344), ('bẩn', 0.7613018751144409), ('chật_chội', 0.7332699298858643), ('chật', 0.730948805809021), ('dơ', 0.7281186580657959), ('củ', 0.7233487367630005), ('bí', 0.7185693383216858), ('cũ_kĩ', 0.7178505659103394), ('tuy_vậy', 0.7023990154266357), ('có_vẻ', 0.6997995376586914)]\n"
          ]
        }
      ],
      "source": [
        "print(word_model.wv.most_similar('xấu'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmQfLpEOx_7_",
        "outputId": "63f992c2-a7c6-4eac-cef2-4026bd10fc1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 1283, 300)         10902900  \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 300)               721200    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 5)                 1505      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,625,605\n",
            "Trainable params: 722,705\n",
            "Non-trainable params: 10,902,900\n",
            "_________________________________________________________________\n",
            "Epoch 1/14\n",
            "298/298 [==============================] - 201s 657ms/step - loss: 0.8448 - acc: 0.6431\n",
            "Epoch 2/14\n",
            "298/298 [==============================] - 197s 663ms/step - loss: 0.7264 - acc: 0.6832\n",
            "Epoch 3/14\n",
            "298/298 [==============================] - 197s 661ms/step - loss: 0.6717 - acc: 0.7094\n",
            "Epoch 4/14\n",
            "298/298 [==============================] - 197s 661ms/step - loss: 0.6249 - acc: 0.7314\n",
            "Epoch 5/14\n",
            "298/298 [==============================] - 197s 663ms/step - loss: 0.5664 - acc: 0.7566\n",
            "Epoch 6/14\n",
            "298/298 [==============================] - 197s 661ms/step - loss: 0.4979 - acc: 0.7922\n",
            "Epoch 7/14\n",
            "298/298 [==============================] - 197s 661ms/step - loss: 0.4176 - acc: 0.8296\n",
            "Epoch 8/14\n",
            "298/298 [==============================] - 197s 661ms/step - loss: 0.3315 - acc: 0.8702\n",
            "Epoch 9/14\n",
            "298/298 [==============================] - 197s 661ms/step - loss: 0.2538 - acc: 0.9062\n",
            "Epoch 10/14\n",
            "298/298 [==============================] - 197s 661ms/step - loss: 0.1881 - acc: 0.9327\n",
            "Epoch 11/14\n",
            "298/298 [==============================] - 197s 661ms/step - loss: 0.1380 - acc: 0.9518\n",
            "Epoch 12/14\n",
            "298/298 [==============================] - 197s 661ms/step - loss: 0.1057 - acc: 0.9645\n",
            "Epoch 13/14\n",
            "298/298 [==============================] - 198s 663ms/step - loss: 0.0888 - acc: 0.9697\n",
            "Epoch 14/14\n",
            "298/298 [==============================] - 197s 661ms/step - loss: 0.0768 - acc: 0.9742\n",
            "133/133 [==============================] - 16s 119ms/step - loss: 1.3098 - acc: 0.6888\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[1.3098299503326416, 0.6887682676315308]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Embedding\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(word_model.wv.vocab)+1,300,input_length=X.shape[1],weights=[embedding_matrix],trainable=False))\n",
        "model.add(LSTM(300,return_sequences=False))\n",
        "model.add(Dense(y.shape[1],activation=\"softmax\"))\n",
        "model.summary()\n",
        "model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=['acc'])\n",
        "\n",
        "batch = 128\n",
        "epochs = 14\n",
        "model.fit(X_train,y_train,batch,epochs)\n",
        "model.evaluate(X_test,y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LiklcTokA0ha"
      },
      "outputs": [],
      "source": [
        "test = []\n",
        "with open(f\"/content/sentiment_analysis_test_unlabel.v1.0.txt\", encoding=\"utf8\") as f:\n",
        "  data_set = f.read().splitlines()\n",
        "  for data in data_set:\n",
        "    data = text_preprocess(data)\n",
        "    test.append(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWqFsfGeyADf",
        "outputId": "16b4994f-c305-40fe-ee01-859e0c881a0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "__label__trung_binh\n"
          ]
        }
      ],
      "source": [
        "seq = tokenizer.texts_to_sequences(test)\n",
        "padded = pad_sequences(seq, count)\n",
        "pred = model.predict(padded)\n",
        "labels = ['__label__kem', '__label__rat_kem', '__label__tot', '__label__trung_binh', '__label__xuat_sac']\n",
        "y = pd.get_dummies(label)\n",
        "print(labels[np.argmax(pred[1])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "f2JWiWtyDRUh"
      },
      "outputs": [],
      "source": [
        "for p in pred:\n",
        "    ff = open(f\"/content/result-multi.txt\", \"a\", encoding=\"utf8\")\n",
        "    ff.write(labels[np.argmax(p)] + '\\n')\n",
        "    ff.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "LTSM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
