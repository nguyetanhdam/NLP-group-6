{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9pjy1q6fHSt",
        "outputId": "f9b26313-f117-49e2-cd43-c795f9f0967f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: underthesea in /usr/local/lib/python3.10/dist-packages (6.8.0)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from underthesea) (8.1.7)\n",
            "Requirement already satisfied: python-crfsuite>=0.9.6 in /usr/local/lib/python3.10/dist-packages (from underthesea) (0.9.9)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from underthesea) (3.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from underthesea) (4.66.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from underthesea) (2.31.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.3.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.2.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from underthesea) (6.0.1)\n",
            "Requirement already satisfied: underthesea-core==1.0.4 in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.0.4)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->underthesea) (2023.6.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2023.7.22)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.11.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (3.2.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.11.3)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install underthesea\n",
        "!pip install imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfCd_Qfs2Q0c",
        "outputId": "ce259b34-e7ad-47a1-dd12-6160aa01fb61"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (1.3.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fNgy0x5ifXm-"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import regex\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import LinearSVC\n",
        "from underthesea import word_tokenize\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
        "import unidecode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OOe4sLv1fawj"
      },
      "outputs": [],
      "source": [
        "with open(f\"/stop_word_vn.txt\", encoding=\"utf8\") as f:\n",
        "    stop_word_pre = f.read().splitlines()\n",
        "\n",
        "def no_accent(s):\n",
        "    s = regex.sub(r'[àáạảãâầấậẩẫăằắặẳẵ]', 'a', s)\n",
        "    s = regex.sub(r'[ÀÁẠẢÃĂẰẮẶẲẴÂẦẤẬẨẪ]', 'A', s)\n",
        "    s = regex.sub(r'[èéẹẻẽêềếệểễ]', 'e', s)\n",
        "    s = regex.sub(r'[ÈÉẸẺẼÊỀẾỆỂỄ]', 'E', s)\n",
        "    s = regex.sub(r'[òóọỏõôồốộổỗơờớợởỡ]', 'o', s)\n",
        "    s = regex.sub(r'[ÒÓỌỎÕÔỒỐỘỔỖƠỜỚỢỞỠ]', 'O', s)\n",
        "    s = regex.sub(r'[ìíịỉĩ]', 'i', s)\n",
        "    s = regex.sub(r'[ÌÍỊỈĨ]', 'I', s)\n",
        "    s = regex.sub(r'[ùúụủũưừứựửữ]', 'u', s)\n",
        "    s = regex.sub(r'[ƯỪỨỰỬỮÙÚỤỦŨ]', 'U', s)\n",
        "    s = regex.sub(r'[ỳýỵỷỹ]', 'y', s)\n",
        "    s = regex.sub(r'[ỲÝỴỶỸ]', 'Y', s)\n",
        "    s = regex.sub(r'[Đ]', 'D', s)\n",
        "    s = regex.sub(r'[đ]', 'd', s)\n",
        "    return s\n",
        "\n",
        "\n",
        "stop_word = set()\n",
        "for a in stop_word_pre:\n",
        "  stop_word.add(a)\n",
        "  stop_word.add(no_accent(a))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "o1jr5saHfukN"
      },
      "outputs": [],
      "source": [
        "MULTI_DIRECT = '/sentiment_analysis_train.v1.0.txt'\n",
        "VN_DATA_DIRECT = '/data_vn.txt'\n",
        "\n",
        "\n",
        "def remove_character_not_ness(document):\n",
        "    document = regex.sub(r'[^\\s\\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ_]', ' ', document)\n",
        "    document = regex.sub(r'[^\\D]', ' ', document)\n",
        "    document = regex.sub(r'\\s+', ' ', document).strip()\n",
        "    return document\n",
        "\n",
        "def remove_stopwords(document):\n",
        "    words = []\n",
        "    for word in document.strip().split():\n",
        "        if word not in stop_word:\n",
        "            words.append(word)\n",
        "    return ' '.join(words)\n",
        "\n",
        "def specify_preprocess(document):\n",
        "    document = document.lower()\n",
        "    document = document.replace('1*', ' một_sao ')\n",
        "    document = document.replace('2*', ' hai_sao ')\n",
        "    document = document.replace('3*', ' ba_sao ')\n",
        "    document = document.replace('4*', ' bốn_sao ')\n",
        "    document = document.replace('5*', ' năm_sao ')\n",
        "    document = document.replace('1 *', ' một_sao ')\n",
        "    document = document.replace('2 *', ' hai_sao ')\n",
        "    document = document.replace('3 *', ' ba_sao ')\n",
        "    document = document.replace('4 *', ' bốn_sao ')\n",
        "    document = document.replace('5 *', ' năm_sao ')\n",
        "    document = document.replace('1 sao', ' một_sao ')\n",
        "    document = document.replace('2 sao', ' hai_sao ')\n",
        "    document = document.replace('3 sao', ' ba_sao ')\n",
        "    document = document.replace('4 sao', ' bốn_sao ')\n",
        "    document = document.replace('5 sao', ' năm_sao ')\n",
        "    document = document.replace('1sao', ' một_sao ')\n",
        "    document = document.replace('2sao', ' hai_sao ')\n",
        "    document = document.replace('3sao', ' ba_sao ')\n",
        "    document = document.replace('4sao', ' bốn_sao ')\n",
        "    document = document.replace('5sao', ' năm_sao ')\n",
        "    document = document.replace('một sao', 'một_sao')\n",
        "    document = document.replace('hai sao', 'hai_sao')\n",
        "    document = document.replace('ba sao', 'ba_sao')\n",
        "    document = document.replace('bốn sao', 'bốn_sao')\n",
        "    document = document.replace('năm sao', 'năm_sao')\n",
        "    return document\n",
        "\n",
        "def text_preprocess(document):\n",
        "    document = document.lower()\n",
        "    document = remove_character_not_ness(document)\n",
        "    document = word_tokenize(document, format=\"text\")\n",
        "    # document = remove_stopwords(document)\n",
        "    return document\n",
        "\n",
        "\n",
        "def load_training_data(\n",
        "        directory\n",
        "):\n",
        "    text = []\n",
        "    label = []\n",
        "    with open(f\"{directory}\", encoding=\"utf8\") as f:\n",
        "        data_set = f.read().splitlines()\n",
        "        for data in data_set:\n",
        "            document = data\n",
        "            tmp = document.split(\" \", 1)\n",
        "            if directory == VN_DATA_DIRECT:\n",
        "              tmp[1] = specify_preprocess(tmp[1])\n",
        "            tmp[1] = text_preprocess(tmp[1])\n",
        "            label.append(tmp[0])\n",
        "            text.append(tmp[1])\n",
        "            s = no_accent(tmp[1]).replace('_', ' ')\n",
        "            if tmp[1] != s:\n",
        "                label.append(tmp[0])\n",
        "                text.append(s)\n",
        "    print(\"Text after processing:\")\n",
        "    print(*text[:10], sep='\\n')\n",
        "    return text, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qq2wTzE7fcbj"
      },
      "outputs": [],
      "source": [
        "def train(text, label, predict_direct, test_direct):\n",
        "    # vectorizer = TfidfVectorizer(ngram_range=(1, 10),max_df=0.8,use_idf=False, sublinear_tf=True)\n",
        "    # text = vectorizer.fit_transform(text)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(text, label, test_size=0.1, random_state=101)\n",
        "    # over_sampler = RandomOverSampler(random_state=42)\n",
        "    # X_train, y_train = over_sampler.fit_resample(X_train, y_train)\n",
        "    label_encoder = LabelEncoder()\n",
        "    label_encoder.fit(y_train)\n",
        "    y_train = label_encoder.transform(y_train)\n",
        "    y_test = label_encoder.transform(y_test)\n",
        "\n",
        "    start_time = time.time()\n",
        "    text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1, 5),\n",
        "                                                  max_df=0.8,\n",
        "                                                  )),\n",
        "                         ('tfidf', TfidfTransformer()),\n",
        "                         ('clf', LinearSVC())\n",
        "                         ])\n",
        "    text_clf = text_clf.fit(X_train, y_train)\n",
        "\n",
        "    train_time = time.time() - start_time\n",
        "    print('Done training SVM in', train_time, 'seconds.')\n",
        "\n",
        "    y_pred = text_clf.predict(X_test)\n",
        "    print('SVM, Accuracy =', np.mean(y_pred == y_test))\n",
        "\n",
        "    y_pred = text_clf.predict(X_test)\n",
        "    print(classification_report(y_test, y_pred, target_names=list(label_encoder.classes_)))\n",
        "\n",
        "    test = []\n",
        "    with open(f\"{test_direct}\", encoding=\"utf8\") as f:\n",
        "      data_set = f.read().splitlines()\n",
        "      for data in data_set:\n",
        "        if predict_direct == VN_DIRECT_PREDICT:\n",
        "           data = specify_preprocess(data)\n",
        "        data = text_preprocess(data)\n",
        "        test.append(data)\n",
        "\n",
        "    label = text_clf.predict(test)\n",
        "    predict = list(label_encoder.inverse_transform(label))\n",
        "    print(predict[0:10])\n",
        "    for text in predict:\n",
        "      ff = open(f\"{predict_direct}\", \"a\", encoding=\"utf8\")\n",
        "      ff.write(text + '\\n')\n",
        "      ff.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1uW_deKlfe-M"
      },
      "outputs": [],
      "source": [
        "MULTI_DIRECT_TEST = '/sentiment_analysis_test_unlabel.v1.0.txt'\n",
        "VN_DIRECT_TEST = '/test_vn.txt'\n",
        "MULTI_DIRECT_PREDICT = '/result_multi.txt'\n",
        "VN_DIRECT_PREDICT = '/result_vn.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWBpEu20fhLj",
        "outputId": "eb96d562-2e62-4d82-87bb-e7a70568ef03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text after processing:\n",
            "good ratio price service good advices for the national_park and_village tốt\n",
            "good ratio price service good advices for the national park and village tot\n",
            "trang_thiết_bị vệ_sinh hơi cũ vệ_sinh sạch_sẽ nhân_viên thân_thiện địa_điểm tốt một khách_sạn sạch_sẽ thoải_mái dễ_chịu địa_điểm tốt\n",
            "trang thiet bi ve sinh hoi cu ve sinh sach se nhan vien than thien dia diem tot mot khach san sach se thoai mai de chiu dia diem tot\n",
            "friendly staff helpful for booking tours room was_clean and_quiet perfect location for ke_bang national park caves\n",
            "friendly staff helpful for booking tours room was clean and quiet perfect location for ke bang national park caves\n",
            "quá ồn_ào thất_vọng\n",
            "qua on ao that vong\n",
            "nice place to be to get around staff really takes care of_you perfect stay super staff\n",
            "nice place to be to get around staff really takes care of you perfect stay super staff\n",
            "Multilingual test\n",
            "Done training SVM in 52.40573000907898 seconds.\n",
            "SVM, Accuracy = 0.7350101055468223\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "       __label__kem       0.71      0.48      0.57       191\n",
            "   __label__rat_kem       1.00      0.24      0.38        34\n",
            "       __label__tot       0.72      0.86      0.78      2151\n",
            "__label__trung_binh       0.70      0.60      0.65       868\n",
            "  __label__xuat_sac       0.81      0.67      0.73      1209\n",
            "\n",
            "           accuracy                           0.74      4453\n",
            "          macro avg       0.79      0.57      0.62      4453\n",
            "       weighted avg       0.74      0.74      0.73      4453\n",
            "\n",
            "['__label__trung_binh', '__label__trung_binh', '__label__trung_binh', '__label__kem', '__label__tot', '__label__trung_binh', '__label__tot', '__label__trung_binh', '__label__tot', '__label__tot']\n"
          ]
        }
      ],
      "source": [
        "train_text, train_label = load_training_data(MULTI_DIRECT)\n",
        "print(\"Multilingual test\")\n",
        "train(train_text, train_label, MULTI_DIRECT_PREDICT, MULTI_DIRECT_TEST)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7H2SVhF5kEvp",
        "outputId": "d0c52137-f9cd-47f5-c3a7-267f33afc5f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text after processing:\n",
            "tỷ_lệ tốt giá dịch_vụ lời khuyên tốt cho vườn_quốc_gia và làng tốt\n",
            "ty le tot gia dich vu loi khuyen tot cho vuon quoc gia va lang tot\n",
            "trang_thiết_bị vệ_sinh hơi cũ vệ_sinh sạch_sẽ nhân_viên thân_thiện địa_điểm tốt một khách_sạn sạch_sẽ thoải_mái dễ_chịu địa_điểm tốt\n",
            "trang thiet bi ve sinh hoi cu ve sinh sach se nhan vien than thien dia diem tot mot khach san sach se thoai mai de chiu dia diem tot\n",
            "nhân_viên thân_thiện hữu_ích để đặt tour du_lịch phòng sạch_sẽ và yên_tĩnh vị_trí hoàn_hảo cho hang_động công_viên quốc_gia ke_bang\n",
            "nhan vien than thien huu ich de dat tour du lich phong sach se va yen tinh vi tri hoan hao cho hang dong cong vien quoc gia ke bang\n",
            "quá ồn_ào thất_vọng\n",
            "qua on ao that vong\n",
            "nơi tốt_đẹp để có được xung_quanh nhân_viên thực_sự chăm_sóc bạn ở lại hoàn_hảo siêu_nhân_viên\n",
            "noi tot dep de co duoc xung quanh nhan vien thuc su cham soc ban o lai hoan hao sieu nhan vien\n",
            "After translate\n",
            "Done training SVM in 57.52921533584595 seconds.\n",
            "SVM, Accuracy = 0.6921739130434783\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "       __label__kem       0.68      0.40      0.51       220\n",
            "   __label__rat_kem       1.00      0.27      0.43        44\n",
            "       __label__tot       0.68      0.82      0.74      2123\n",
            "__label__trung_binh       0.65      0.55      0.59       879\n",
            "  __label__xuat_sac       0.76      0.65      0.70      1334\n",
            "\n",
            "           accuracy                           0.69      4600\n",
            "          macro avg       0.75      0.54      0.59      4600\n",
            "       weighted avg       0.70      0.69      0.69      4600\n",
            "\n",
            "['__label__trung_binh', '__label__trung_binh', '__label__tot', '__label__kem', '__label__tot', '__label__trung_binh', '__label__tot', '__label__trung_binh', '__label__tot', '__label__tot']\n"
          ]
        }
      ],
      "source": [
        "train_text, train_label = load_training_data(VN_DATA_DIRECT)\n",
        "print(\"After translate\")\n",
        "train(train_text, train_label, VN_DIRECT_PREDICT, VN_DIRECT_TEST)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}